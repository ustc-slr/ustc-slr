
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":["lihq"],"categories":null,"content":"Houqiang Li is a professor and the Vice Dean of the School of Information Science and Technology, University of Science and Technology of China (USTC), and the Director of MOE-Microsoft Key Laboratory of Multimedia Computing and Communication. He received the B.S., M.Eng., and Ph.D. degrees in electronic engineering from USTC in 1992, 1997, and 2000, respectively. His research interests include image/video coding, image/video analysis, computer vision, deep learning and reinforcement learning, etc.. He has authored and co-authored over 300 papers in journals and conferences, and holds over 60 granted patents.\nHe is a fellow of IEEE. He is the associate editor (AE) of IEEE TMM, and served as the AE of IEEE TCSVT from 2010 to 2013. He served as the TPC Co-Chair of VCIP 2010, and he will serve as the General Co-Chair of ICME 2021. He is the winner of National Science Funds (NSFC) for Distinguished Young Scientists, the Distinguished Professor of Changjiang Scholars Program of China, the Leading Scientist of Ten Thousand Talent Program of China. He is the recipient of National Technological Invention Award of China (second class) in 2019 and the recipient of National Natural Science Award of China (second class) in 2015. He was the recipient of the Best Paper Award for VCIP 2012, the recipient of the Best Paper Award for ICIMCS 2012, and the recipient of the Best Paper Award for ACM MUM 2011. He received the award for the Excellent Ph.D Supervisor of Chinese Academy of Sciences (CAS) for four times from 2013 to 2016.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"5d07663258f279f1fad63d43dc9b5d4a","permalink":"https://ustc-slr.github.io/author/houqiang-li/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/houqiang-li/","section":"authors","summary":"Houqiang Li is a professor and the Vice Dean of the School of Information Science and Technology, University of Science and Technology of China (USTC), and the Director of MOE-Microsoft Key Laboratory of Multimedia Computing and Communication.","tags":null,"title":"Houqiang Li","type":"authors"},{"authors":["zhwg"],"categories":null,"content":"Wengang Zhou received the B.E. degree in electronic information engineering from Wuhan University, China, in 2006, and the Ph.D. degree in electronic engineering and information science from the University of Science and Technology of China (USTC), China, in 2011. From September 2011 to September 2013, he worked as a postdoc researcher in Computer Science Department at the University of Texas at San Antonio. He is currently a Professor at the EEIS Department, USTC.\nHis research interests include multimedia information retrieval, computer vision, and computer game. In those fields, he has published over 100 papers in IEEE/ACM Transactions and CCF Tier-A International Conferences. He is the winner of National Science Funds of China (NSFC) for Excellent Young Scientists. He is the recepient of the Best Paper Award for ICIMCS 2012. He received the award for the Excellent Ph.D Supervisor of Chinese Society of Image and Graphics (CSIG) in 2021, and the award for the Excellent Ph.D Supervisor of Chinese Academy of Sciences (CAS) in 2022. He won the First Class Wu-Wenjun Award for Progress in Artificial Intelligence Technology in 2021. He served as the publication chair of IEEE ICME 2021 and won 2021 ICME Outstanding Service Award. He is currently an Associate Editor and a Lead Guest Editor of IEEE Transactions on Multimeida.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"1673f14df281a2de6e9c981ab547f11e","permalink":"https://ustc-slr.github.io/author/wengang-zhou/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/wengang-zhou/","section":"authors","summary":"Wengang Zhou received the B.E. degree in electronic information engineering from Wuhan University, China, in 2006, and the Ph.D. degree in electronic engineering and information science from the University of Science and Technology of China (USTC), China, in 2011.","tags":null,"title":"Wengang Zhou","type":"authors"},{"authors":null,"categories":null,"content":"IEEE TPAMI 2023\nHand gesture serves as a crucial role during the expression of sign language. Current deep learning based methods for sign language understanding (SLU) are prone to over-fitting due to insufficient sign data resource and suffer limited interpretability. In this paper, we propose the first self-supervised pre-trainable SignBERT+ framework with model-aware hand prior incorporated. In our framework, the hand pose is regarded as a visual token, which is derived from an off-the-shelf detector. Each visual token is embedded with gesture state and spatial-temporal position encoding. To take full advantage of current sign data resource, we first perform self-supervised learning to model its statistics. To this end, we design multi-level masked modeling strategies (joint, frame and clip) to mimic common failure detection cases. Jointly with these masked modeling strategies, we incorporate model-aware hand prior to better capture hierarchical context over the sequence. After the pre-training, we carefully design simple yet effective prediction heads for downstream tasks. To validate the effectiveness of our framework, we perform extensive experiments on three main SLU tasks, involving isolated and continuous sign language recognition (SLR), and sign language translation (SLT). Experimental results demonstrate the effectiveness of our method, achieving new state-of-the-art performance with a notable gain.\nPlease refer to this project page for more details. ","date":1682467200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1682467200,"objectID":"836ec6aaba3d68a47215bb4230a5a33e","permalink":"https://ustc-slr.github.io/highlight_news/23-05-02-signbert+/","publishdate":"2023-04-26T00:00:00Z","relpermalink":"/highlight_news/23-05-02-signbert+/","section":"highlight_news","summary":"IEEE TPAMI 2023\n","tags":null,"title":"SignBERT+: Hand-model-aware Self-supervised Pre-training for Sign Language Understanding","type":"highlight_news"},{"authors":["Hezhen Hu","Weichao Zhao","Wengang Zhou","Houqiang Li"],"categories":null,"content":"","date":1682467200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1682467200,"objectID":"1e076c3dc6b16f4f65cfda2b1a9a521c","permalink":"https://ustc-slr.github.io/publication/hu2023signbert/","publishdate":"2023-04-26T00:00:00Z","relpermalink":"/publication/hu2023signbert/","section":"publication","summary":"Hand gesture serves as a crucial role during the expression of sign language. Current deep learning based methods for sign language understanding (SLU) are prone to over-fitting due to insufficient sign data resource and suffer limited interpretability. In this paper, we propose the first self-supervised pre-trainable SignBERT+ framework with model-aware hand prior incorporated. In our framework, the hand pose is regarded as a visual token, which is derived from an off-the-shelf detector. Each visual token is embedded with gesture state and spatial-temporal position encoding. To take full advantage of current sign data resource, we first perform self-supervised learning to model its statistics. To this end, we design multi-level masked modeling strategies (joint, frame and clip) to mimic common failure detection cases. Jointly with these masked modeling strategies, we incorporate model-aware hand prior to better capture hierarchical context over the sequence. After the pre-training, we carefully design simple yet effective prediction heads for downstream tasks. To validate the effectiveness of our framework, we perform extensive experiments on three main SLU tasks, involving isolated and continuous sign language recognition (SLR), and sign language translation (SLT). Experimental results demonstrate the effectiveness of our method, achieving new state-of-the-art performance with a notable gain.","tags":["ISLR","CSLR","SLT"],"title":"SignBERT+: Hand-model-aware Self-supervised Pre-training for Sign Language Understanding","type":"publication"},{"authors":["Hezhen Hu","Junfu Pu","Wengang Zhou","Hang Fang","Houqiang Li"],"categories":null,"content":"","date":1681862400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1681862400,"objectID":"9260bffddeb2b0bbf1a3567d8b4729dc","permalink":"https://ustc-slr.github.io/publication/hu2023prior/","publishdate":"2023-04-19T00:00:00Z","relpermalink":"/publication/hu2023prior/","section":"publication","summary":"Continuous sign language recognition (CSLR) aims to map a sign video into a sentence of text words in the same order as the signs. Generally, word error rate (WER), i.e., editing distance, is adopted as the main evaluation metric. Since this metric is not differentiable, current deep-learning-based CSLR methods usually resort to connectionist temporal classification (CTC) loss during optimization, which maximizes the posterior probability over the sequential alignment. Due to the optimization gap between CTC loss and WER, the decoded sequence with the maximum probability in CTC may not be the one with the lowest WER. To tackle this issue, we propose a novel prior-aware cross modality augmentation learning method. In our approach, we first generate the pseudo video-text pair by cross modality editing, i.e., substitution, deletion and insertion on the paired real video-text data. To ensure the pseudo data quality, we guide the editing with both textual grammar prior and visual pose transition consistency prior. In this way, the generated pseudo video and text sentence follow the underlying distribution of the sign language data, and sever as more genuine hard examples for the cross modality representation learning of our CSLR task. Based on the real and generated pseudo data, we optimize our CSLR framework with three loss terms. We evaluate our approach on popular large-scale CSLR datasets and extensive experiments demonstrate the effectiveness of our method.","tags":["CSLR"],"title":"Prior-aware Cross Modality Augmentation Learning for Continuous Sign Language Recognition","type":"publication"},{"authors":null,"categories":null,"content":"AAAI 2023, Oral\nIn this work, we are dedicated to leveraging the BERT pretraining success and modeling the domain-specific statistics to fertilize the sign language recognition (SLR) model. Considering the dominance of hand and body in sign language expression, we organize them as pose triplet units and feed them into the Transformer backbone in a frame-wise manner. Pre-training is performed via reconstructing the masked triplet unit from the corrupted input sequence, which learns the hierarchical correlation context cues among internal and external triplet units. Notably, different from the highly semantic word token in BERT, the pose unit is a low-level signal originally located in continuous space, which prevents the direct adoption of the BERT cross-entropy objective. To this end, we bridge this semantic gap via coupling tokenization of the triplet unit. It adaptively extracts the discrete pseudo label from the pose triplet unit, which represents the semantic gesture/body state. After pre-training, we fine-tune the pretrained encoder on the downstream SLR task, jointly with the newly added task-specific layer. Extensive experiments are conducted to validate the effectiveness of our proposed method, achieving new state-of-the-art performance on all four benchmarks with a notable gain.\nPlease refer to this project page for more details. ","date":1675209600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1675209600,"objectID":"8a3fcd22bdbfe34389bbf2f282996dfa","permalink":"https://ustc-slr.github.io/highlight_news/23-02-01-best/","publishdate":"2023-02-01T00:00:00Z","relpermalink":"/highlight_news/23-02-01-best/","section":"highlight_news","summary":"AAAI 2023, Oral\n","tags":null,"title":"BEST: BERT Pre-Training for Sign Language Recognition with Coupling Tokenization","type":"highlight_news"},{"authors":["Weichao Zhao","Hezhen Hu","Wengang Zhou","Jiaxin Shi","Houqiang Li"],"categories":null,"content":"","date":1675209600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1675209600,"objectID":"b0427279f5c9d7211761a46d8072c446","permalink":"https://ustc-slr.github.io/publication/zhao2023best/","publishdate":"2023-02-01T00:00:00Z","relpermalink":"/publication/zhao2023best/","section":"publication","summary":"In this work, we are dedicated to leveraging the BERT pre-training success and modeling the domain-specific statistics to fertilize the sign language recognition (SLR) model. Considering the dominance of hand and body in sign language expression, we organize them as pose triplet units and feed them into the Transformer backbone in a frame-wise manner. Pre-training is performed via reconstructing the masked triplet unit from the corrupted input sequence, which learns the hierarchical correlation context cues among internal and external triplet units. Notably, different from the highly semantic word token in BERT, the pose unit is a low-level signal originally located in continuous space, which prevents the direct adoption of the BERT cross-entropy objective. To this end, we bridge this semantic gap via coupling tokenization of the triplet unit. It adaptively extracts the discrete pseudo label from the pose triplet unit, which represents the semantic gesture/body state. After pre-training, we fine-tune the pre-trained encoder on the downstream SLR task, jointly with the newly added task-specific layer. Extensive experiments are conducted to validate the effectiveness of our proposed method, achieving new state-of-the-art performance on all four benchmarks with a notable gain.","tags":["ISLR"],"title":"BEST: BERT Pre-Training for Sign Language Recognition with Coupling Tokenization","type":"publication"},{"authors":null,"categories":null,"content":"IEEE TMM 2022\nCurrent continuous sign language recognition systems generally target on a single language. When it comes to the multilingual problem, existing solutions often build separate models based on the same network and then train them with their corresponding sign language corpora. Observing that different sign languages share some low-level visual patterns, we argue that it is beneficial to optimize the recognition model in a collaborative way. With this motivation, we propose the first unified framework for multilingual continuous sign language recognition. Our framework consists of a shared visual encoder for visual information encoding, multiple language-dependent sequential modules for long-range temporal dependency learning aimed at different languages, and a universal sequential module to learn the commonality of all languages. An additional language embedding is introduced to distinguish different languages within the shared temporal encoders. Further, we present a max-probability decoding method to obtain the alignment between sign videos and sign words for visual encoder refinement. We evaluate our approach on three continuous sign language recognition benchmarks, i.e., RWTH-PHOENIX-Weather, CSL and GSL-SD. The experimental results reveal that our method outperforms the individually trained recognition models. Our method also demonstrates better performance compared with state-of-the-art algorithms.\nPlease refer to this project page for more details. ","date":1669420800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669420800,"objectID":"9970680dce8ab2872f61f18ed2ef0949","permalink":"https://ustc-slr.github.io/highlight_news/23-05-02-coslr/","publishdate":"2022-11-26T00:00:00Z","relpermalink":"/highlight_news/23-05-02-coslr/","section":"highlight_news","summary":"IEEE TMM 2022\n","tags":null,"title":"Collaborative Multilingual Continuous Sign Language Recognition: A Unified Framework","type":"highlight_news"},{"authors":["Hezhen Hu","Junfu Pu","Wengang Zhou","Houqiang Li"],"categories":null,"content":"","date":1668729600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668729600,"objectID":"9d32a2c1d579fa1f73ef0320f3076201","permalink":"https://ustc-slr.github.io/publication/hu2022collaborative/","publishdate":"2022-11-18T00:00:00Z","relpermalink":"/publication/hu2022collaborative/","section":"publication","summary":"Current continuous sign language recognition systems generally target on a single language. When it comes to the multilingual problem, existing solutions often build separate models based on the same network and then train them with their corresponding sign language corpora. Observing that different sign languages share some low-level visual patterns, we argue that it is beneficial to optimize the recognition model in a collaborative way. With this motivation, we propose the first unified framework for multilingual continuous sign language recognition. Our framework consists of a shared visual encoder for visual information encoding, multiple language-dependent sequential modules for long-range temporal dependency learning aimed at different languages, and a universal sequential module to learn the commonality of all languages. An additional language embedding is introduced to distinguish different languages within the shared temporal encoders. Further, we present a max-probability decoding method to obtain the alignment between sign videos and sign words for visual encoder refinement. We evaluate our approach on three continuous sign language recognition benchmarks, i.e., RWTH-PHOENIX-Weather, CSL and GSL-SD. The experimental results reveal that our method outperforms the individually trained recognition models. Our method also demonstrates better performance compared with state-of-the-art algorithms.","tags":["CSLR"],"title":"Collaborative Multilingual Continuous Sign Language Recognition: A Unified Framework","type":"publication"},{"authors":["Hezhen Hu","Weilun Wang","Wengang Zhou","Houqiang Li"],"categories":null,"content":"","date":1667260800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1667260800,"objectID":"e4393f9da49ce0efa2d8d4a541d39340","permalink":"https://ustc-slr.github.io/publication/hu2022handobject/","publishdate":"2022-11-01T00:00:00Z","relpermalink":"/publication/hu2022handobject/","section":"publication","summary":"In this work, we are dedicated to a new task, i.e., hand-object interaction image generation, which aims to conditionally generate the hand-object image under the given hand, object and their interaction status. This task is challenging and research-worthy in many potential application scenarios, such as AR/VR games and online shopping, etc. To address this problem, we propose a novel HOGAN framework, which utilizes the expressive model-aware hand-object representation and leverages its inherent topology to build the unified surface space. In this space, we explicitly consider the complex self- and mutual occlusion during interaction. During final image synthesis, we consider different characteristics of hand and object and generate the target image in a split-and-combine manner. For evaluation, we build a comprehensive protocol to access both the fidelity and structure preservation of the generated image. Extensive experiments on two large-scale datasets, i.e., HO3Dv3 and DexYCB, demonstrate the effectiveness and superiority of our framework both quantitatively and qualitatively.","tags":["Hand Generation"],"title":"Hand-Object Interaction Image Generation","type":"publication"},{"authors":null,"categories":null,"content":"","date":1666569600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666569600,"objectID":"1e819a4757d703106449dbc0c799e22f","permalink":"https://ustc-slr.github.io/highlights/","publishdate":"2022-10-24T00:00:00Z","relpermalink":"/highlights/","section":"","summary":"","tags":null,"title":"Highlights","type":"widget_page"},{"authors":["Hezhen Hu","Weichao Zhao","Wengang Zhou","Yuechen Wang","Houqiang Li"],"categories":null,"content":"","date":1633046400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633046400,"objectID":"995b11f82a0fcc52fb5f86aeadb336e7","permalink":"https://ustc-slr.github.io/publication/hu2021signbert/","publishdate":"2021-10-01T00:00:00Z","relpermalink":"/publication/hu2021signbert/","section":"publication","summary":"Hand gesture serves as a critical role in sign language. Current deep-learning-based sign language recognition (SLR) methods may suffer insufficient interpretability and overfitting due to limited sign data sources. In this paper, we introduce the first self-supervised pre-trainable SignBERT with incorporated hand prior for SLR. SignBERT views the hand pose as a visual token, which is derived from an off-the-shelf pose extractor. The visual tokens are then embedded with gesture state, temporal and hand chirality information. To take full advantage of available sign data sources, SignBERT first performs self-supervised pre-training by masking and reconstructing visual tokens. Jointly with several mask modeling strategies, we attempt to incorporate hand prior in a model-aware method to better model hierarchical context over the hand sequence. Then with the prediction head added, SignBERT is fine-tuned to perform the downstream SLR task. To validate the effectiveness of our method on SLR, we perform extensive experiments on four public benchmark datasets, i.e., NMFs-CSL, SLR500, MSASL and WLASL. Experiment results demonstrate the effectiveness of both self-supervised learning and imported hand prior. Furthermore, we achieve state-of-the-art performance on all benchmarks with a notable gain.","tags":["ISLR"],"title":"SignBERT: Pre-Training of Hand-Model-Aware Representation for Sign Language Recognition","type":"publication"},{"authors":["Hezhen Hu","Wengang Zhou","Junfu Pu","Houqiang Li"],"categories":null,"content":"","date":1626912000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1626912000,"objectID":"6ad7046d5d7a2077460495287c514b27","permalink":"https://ustc-slr.github.io/publication/hu2021global/","publishdate":"2021-07-22T00:00:00Z","relpermalink":"/publication/hu2021global/","section":"publication","summary":"Sign language recognition (SLR) is a challenging problem, involving complex manual features (i.e., hand gestures) and fine-grained non-manual features (NMFs) (i.e., facial expression, mouth shapes, etc.). Although manual features are dominant, non-manual features also play an important role in the expression of a sign word. Specifically, many sign words convey different meanings due to non-manual features, even though they share the same hand gestures. This ambiguity introduces great challenges in the recognition of sign words. To tackle the above issue, we propose a simple yet effective architecture called Global-Local Enhancement Network (GLE-Net), including two mutually promoted streams toward different crucial aspects of SLR. Of the two streams, one captures the global contextual relationship, while the other stream captures the discriminative fine-grained cues. Moreover, due to the lack of datasets explicitly focusing on this kind of feature, we introduce the first non-manual-feature-aware isolated Chinese sign language dataset (NMFs-CSL) with a total vocabulary size of 1,067 sign words in daily life. Extensive experiments on NMFs-CSL and SLR500 datasets demonstrate the effectiveness of our method.","tags":["ISLR"],"title":"Global-Local Enhancement Network for NMF-Aware Sign Language Recognition","type":"publication"},{"authors":null,"categories":null,"content":"CVPR 2021\nDespite existing pioneering works on sign language translation (SLT), there is a non-trivial obstacle, i.e., the limited quantity of parallel sign-text data. To tackle this parallel data bottleneck, we propose a sign back-translation (SignBT) approach, which incorporates massive spoken language texts into SLT training. With a text-to-gloss translation model, we first back-translate the monolingual text to its gloss sequence. Then, the paired sign sequence is generated by splicing pieces from an estimated gloss-to-sign bank at the feature level. Finally, the synthetic parallel data serves as a strong supplement for the end-to-end training of the encoder-decoder SLT framework. To promote the SLT research, we further contribute CSLDaily, a large-scale continuous SLT dataset. It provides both spoken language translations and gloss-level annotations. The topic revolves around people’s daily lives (e.g., travel, shopping, medical care), the most likely SLT application scenario. Extensive experimental results and analysis of SLT methods are reported on CSL-Daily. With the proposed sign back-translation method, we obtain a substantial improvement over previous state-of-the-art SLT methods.\nPlease refer to this project page for more details. ","date":1623801600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623801600,"objectID":"761f1b235e3ff12cb2dea3b4d03ab16e","permalink":"https://ustc-slr.github.io/highlight_news/21-02-01-signbt/","publishdate":"2021-06-16T00:00:00Z","relpermalink":"/highlight_news/21-02-01-signbt/","section":"highlight_news","summary":"CVPR 2021\n","tags":null,"title":"Improving Sign Language Translation with Monolingual Data by Sign Back-Translation","type":"highlight_news"},{"authors":["Jian Zhao","Weizhen Qi","Wengang Zhou","Nan Duan","Ming Zhou","Houqiang Li"],"categories":null,"content":"","date":1623024000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623024000,"objectID":"54b365ce1310d7e33505b8e7245b391d","permalink":"https://ustc-slr.github.io/publication/zhao2021conditional/","publishdate":"2021-06-07T00:00:00Z","relpermalink":"/publication/zhao2021conditional/","section":"publication","summary":"Sign Language Translation (SLT) aims to generate spoken language translations from sign language videos. Currently, the available sign language datasets are relatively too small to learn the linguistic properties of spoken language. In this paper, towards effective SLT, we propose a novel framework which takes the advantage of the spoken language grammar learnt from a large corpus of text sentences. Our framework consists of three key modules word existence verification, conditional sentence generation and cross-modal re-ranking. We first check the existence of words in the vocabulary by a series of binary classification in parallel. After that, the appearing words are assembled and guided by a pretrained spoken language generator to produce multiple candidate sentences in spoken language manner. Last but not least, we select the sentence most semantically similar to the input sign video as the translation result with a crossmodal re-ranking model. We evaluate our framework on two large scale continuous SLT benchmarks, i.e. , CSL and RWTHPHOENIX-Weather 2014 T. Experimental results demonstrate that the proposed framework achieves promising performance on both datasets.","tags":["SLT"],"title":"Conditional Sentence Generation and Cross-Modal Reranking for Sign Language Translation","type":"publication"},{"authors":["Hao Zhou","Wengang Zhou","Yun Zhou","Houqiang Li"],"categories":null,"content":"","date":1613347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613347200,"objectID":"3d04df4fa428946fa5f4c64ad6529ece","permalink":"https://ustc-slr.github.io/publication/zhou2021spatial/","publishdate":"2021-02-15T00:00:00Z","relpermalink":"/publication/zhou2021spatial/","section":"publication","summary":"Despite the recent success of deep learning in video-related tasks, deep models typically focus on the most discriminative features, ignoring other potentially non-trivial and informative contents. Such characteristic heavily constrains their capability to learn implicit visual grammars in sign videos behind the collaboration of different visual cues (i.e., hand shape, facial expression and body posture). To this end, we approach video-based sign language understanding with multi-cue learning and propose a spatial-temporal multi-cue (STMC) network to solve the vision-based sequence learning problem. Our STMC network consists of a spatial multi-cue (SMC) module and a temporal multi-cue (TMC) module. The SMC module learns to spatial representation of different cues with a self-contained pose estimation branch. The TMC module models temporal corrections from intra-cue and inter-cue perspectives to explore the collaboration of multiple cues.A joint optimization strategy and a segmented attention mechanism are designed to make the best of multi-cue sources for SL recognition and translation. To validate the effectiveness, we perform experiments on three large-scale sign language benchmarks PHOENIX-2014, CSL and PHOENIX-2014-T. Experimental results demonstrate that the proposed method achieves new state-of-the-art performance on all three benchmarks.","tags":["CSLR","SLT"],"title":"Spatial-Temporal Multi-Cue Network for Sign Language Recognition and Translation","type":"publication"},{"authors":["Hao Zhou","Wengang Zhou","Weizhen Qi","Junfu Pu","Houqiang Li"],"categories":null,"content":"","date":1612137600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612137600,"objectID":"c9ebbd04d1168e80fa44b42616ed9795","permalink":"https://ustc-slr.github.io/publication/zhou2021improving/","publishdate":"2021-02-01T00:00:00Z","relpermalink":"/publication/zhou2021improving/","section":"publication","summary":"Despite existing pioneering works on sign language translation (SLT), there is a non-trivial obstacle, i.e., the limited quantity of parallel sign-text data. To tackle this parallel data bottleneck, we propose a sign back-translation (SignBT) approach, which incorporates massive spoken language texts into SLT training. With a text-to-gloss translation model, we first back-translate the monolingual text to its gloss sequence. Then, the paired sign sequence is generated by splicing pieces from an estimated gloss-to-sign bank at the feature level. Finally, the synthetic parallel data serves as a strong supplement for the end-to-end training of the encoder-decoder SLT framework. To promote the SLT research, we further contribute CSL-Daily, a large-scale continuous SLT dataset. It provides both spoken language translations and gloss-level annotations. The topic revolves around people's daily lives (e.g., travel, shopping, medical care), the most likely SLT application scenario. Extensive experimental results and analysis of SLT methods are reported on CSL-Daily. With the proposed sign back-translation method, we obtain a substantial improvement over previous state-of-the-art SLT methods.","tags":["SLT"],"title":"Improving sign language translation with monolingual data by sign back-translation","type":"publication"},{"authors":["Hezhen Hu","Weilun Wang","Wengang Zhou","Weichao Zhao","Houqiang Li"],"categories":null,"content":"","date":1612137600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612137600,"objectID":"efb8346849e375dbead00844b7a9041e","permalink":"https://ustc-slr.github.io/publication/hu2021model/","publishdate":"2021-02-01T00:00:00Z","relpermalink":"/publication/hu2021model/","section":"publication","summary":"Hand gesture-to-gesture translation is a significant and interesting problem, which serves as a key role in many applications, such as sign language production. This task involves fine-grained structure understanding of the mapping between the source and target gestures. Current works follow a data-driven paradigm based on sparse 2D joint representation. However, given the insufficient representation capability of 2D joints, this paradigm easily leads to blurry generation results with incorrect structure. In this paper, we propose a novel model-aware gesture-to-gesture translation framework, which introduces hand prior with hand meshes as the intermediate representation. To take full advantage of the structured hand model, we first build a dense topology map aligning the image plane with the encoded embedding of the visible hand mesh. Then, a transformation flow is calculated based on the correspondence of the source and target topology map. During the generation stage, we inject the topology information into generation streams by modulating the activations in a spatially-adaptive manner. Further, we incorporate the source local characteristic to enhance the translated gesture image according to the transformation flow. Extensive experiments on two benchmark datasets have demonstrated that our method achieves new state-of-the-art performance.","tags":["Hand Generation"],"title":"Model-Aware Gesture-to-Gesture Translation","type":"publication"},{"authors":null,"categories":null,"content":"Introduction CSL-Daily is a large-scale continuous SLT dataset. It provides both spoken language translations and gloss-level annotations. The topic revolves around people\u0026#39;s daily lives (e.g., travel, shopping, medical care), the most likely SLT application scenario. Download The CSL-Daily database is released to universities and research institutes for research purpose only. To request the access right to the data resources, please follow the instructions below: Download the CSL-Daily Dataset Release Agreement; Read all items and conditions carefully; Complete it appropriately. Note that the agreement should be signed by a full-time staff member (that is, the student is not acceptable). Please scan the signed agreement, send it to (ustc_vslrg At 126.com) and CC to Prof. Zhou (zhwg At ustc.edu.cn). If you are a student, please also CC to the full-time staff member who sign the agreement. Reference Please cite the following papers if you use CSL-Daily for your research Hao Zhou, Wengang Zhou, Weizhen Qi, Junfu Pu, and Houqiang Li, \u0026#34;Improving Sign Language Translation with Monolingual Data by Sign Back-Translation,\u0026#34; IEEE/CVF International Conference on Computer Vision and Pattern Recognition (CVPR), 2021. Besides, you can refer to the following papers for continuous SLR published by our group: Hezhen Hu, Weichao Zhao, Wengang Zhou, and Houqiang Li, \u0026#34;SignBERT+: Hand-model-aware Self-supervised Pre-training for Sign Language Understanding,\u0026#34; IEEE Trans. Pattern Anal. Mach. Intell. (TPAMI), 2023.\tContact If you have any questions about the dataset and our papers, please feel free to contact us: Wengang Zhou, Professor, USTC, zhwg AT ustc.edu.cn Houqiang Li, Professor, USTC, lihq AT ustc.edu.cn Weichao Zhao, Ph.D Student, USTC, saruka AT mail.ustc.edu.cn ","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"eb6917dab177f2be3ba248a1a78462b6","permalink":"https://ustc-slr.github.io/datasets/2021_csl_daily/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/datasets/2021_csl_daily/","section":"datasets","summary":"","tags":null,"title":"CSL-Daily Dataset","type":"datasets"},{"authors":null,"categories":null,"content":"ACM MM 2020\nContinuous sign language recognition (SLR) deals with unaligned video-text pair and uses the word error rate (WER), i.e., edit distance, as the main evaluation metric. Since it is not differentiable, we usually instead optimize the learning model with the connectionist temporal classification (CTC) objective loss, which maximizes the posterior probability over the sequential alignment. Due to the optimization gap, the predicted sentence with the highest decoding probability may not be the best choice under the WER metric. To tackle this issue, we propose a novel architecture with cross modality augmentation. Specifically, we first augment cross-modal data by simulating the calculation procedure of WER, i.e., substitution, deletion and insertion on both text label and its corresponding video. With these real and generated pseudo video-text pairs, we propose multiple loss terms to minimize the cross modality distance between the video and ground truth label, and make the network distinguish the difference between real and pseudo modalities. The proposed framework can be easily extended to other existing CTC based continuous SLR architectures. Extensive experiments on two continuous SLR benchmarks, i.e., RWTH-PHOENIX-Weather and CSL, validate the effectiveness of our proposed method.\nPlease refer to this project page for more details. ","date":1605830400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605830400,"objectID":"48ac9d3f6a0b632a1cd037a40cfdf5ac","permalink":"https://ustc-slr.github.io/highlight_news/20-11-10-cma/","publishdate":"2020-11-20T00:00:00Z","relpermalink":"/highlight_news/20-11-10-cma/","section":"highlight_news","summary":"ACM MM 2020\n","tags":null,"title":"Boosting Continuous Sign Language Recognition via Cross Modality Augmentation","type":"highlight_news"},{"authors":["Hezhen Hu","Wengang Zhou","Houqiang Li"],"categories":null,"content":"","date":1604188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604188800,"objectID":"204d3e3a838ae1ff67b4453872c8d79d","permalink":"https://ustc-slr.github.io/publication/hu2021hand/","publishdate":"2020-11-01T00:00:00Z","relpermalink":"/publication/hu2021hand/","section":"publication","summary":"Hand gestures play a dominant role in the expression of sign language. Current deep-learning based video sign language recognition (SLR) methods usually follow a data driven paradigm under the supervision of the category label. However, those methods suffer limited interpretability and may encounter the overfitting issue due to limited sign data sources. In this paper, we introduce the hand prior and propose a new hand-model-aware framework for isolated SLR with the modeling hand as the intermediate representation. We first transform the cropped hand sequence into the latent semantic feature. Then the hand model introduces the hand prior and provides a mapping from the semantic feature to the compact hand pose representation. Finally, the inference module enhances the spatio-temporal pose representation and performs the final recognition. Due to the lack of annotation on the hand pose under current sign language datasets, we further guide its learning by utilizing multiple weaklysupervised losses to constrain its spatial and temporal consistency. To validate the effectiveness of our method, we perform extensive experiments on four benchmark datasets, including NMFs-CSL, SLR500, MSASL and WLASL. Experimental results demonstrate that our method achieves stateof-the-art performance on all four popular benchmarks with a notable margin.","tags":["ISLR"],"title":"Hand-Model-Aware Sign Language Recognition","type":"publication"},{"authors":["Chengcheng Wei","Jian Zhao","Wengang Zhou","Houqiang Li"],"categories":null,"content":"","date":1591056000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591056000,"objectID":"faabc9c4fe24b8a2ad19b189dc72e1ab","permalink":"https://ustc-slr.github.io/publication/wei2020semantic/","publishdate":"2020-06-02T00:00:00Z","relpermalink":"/publication/wei2020semantic/","section":"publication","summary":"Sign language recognition (SLR) is a significant and promising technique to facilitate the communication for the hearing-impaired people. In this paper, we are dedicated to weakly supervised continuous SLR, where for each sign video, there are only ordered gloss labels without temporal boundary along frames. To explicitly align video frames to the sign words in a sign video, we propose a novel semantic boundary detection method based on reinforcement learning for accurate continuous SLR. In our approach, we first propose a multi-scale perception scheme to learn discriminative representation for video clips. Then, we formulate the semantic boundary detection as a reinforcement learning problem. We define the state as the feature representation of a video segment, and the action as the determination of the semantic boundary's location. The reward is computed by the quantitative performance metric between the prediction sentence and the ground truth sentence. The policy network is trained with a policy gradient algorithm. Extensive experiments are conducted on CSL Split II and RWTH-PHOENIX-Weather 2014 datasets, and the results demonstrate the effectiveness and superiority of our method.","tags":["CSLR"],"title":"Semantic Boundary Detection With Reinforcement Learning for Continuous Sign Language Recognition","type":"publication"},{"authors":["Junfu Pu","Wengang Zhou","Hezhen Hu","Houqiang Li"],"categories":null,"content":"","date":1590969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590969600,"objectID":"8ac4d54e405c6cdc54c63e0765ec7ff9","permalink":"https://ustc-slr.github.io/publication/pu2020boosting/","publishdate":"2020-06-01T00:00:00Z","relpermalink":"/publication/pu2020boosting/","section":"publication","summary":"Continuous sign language recognition (SLR) deals with unaligned video-text pair and uses the word error rate (WER), i.e., edit distance, as the main evaluation metric. Since it is not differentiable, we usually instead optimize the learning model with the connectionist temporal classification (CTC) objective loss, which maximizes the posterior probability over the sequential alignment. Due to the optimization gap, the predicted sentence with the highest decoding probability may not be the best choice under the WER metric. To tackle this issue, we propose a novel architecture with cross modality augmentation. Specifically, we first augment cross-modal data by simulating the calculation procedure of WER, i.e., substitution, deletion and insertion on both text label and its corresponding video. With these real and generated pseudo video-text pairs, we propose multiple loss terms to minimize the cross modality distance between the video and ground truth label, and make the network distinguish the difference between real and pseudo modalities. The proposed framework can be easily extended to other existing CTC based continuous SLR architectures. Extensive experiments on two continuous SLR benchmarks, i.e., RWTH-PHOENIX-Weather and CSL, validate the effectiveness of our proposed method.","tags":["CSLR"],"title":"Boosting continuous sign language recognition via cross modality augmentation","type":"publication"},{"authors":null,"categories":null,"content":" Introduction This NMFs-CSL dataset explicitly emphasizes the importance of the non-manual features in sign language. It contains 1,067 Chinese sign words (610 confusing words (0~609), 457 normal words (610~1066)). It is collected with portable cameras on the mobile phones. The sign videos are recorded with 30fps. Each instance in the dataset contains RGB videos. Download (*Important ) This NMFs-CSL dataset contains 1,067 Chinese sign words. It is released to universities and research institutes for research purpose only. To request the access right to the data resources, please follow the instructions below: Download the NMFs-CSL Dataset Release Agreement; Read it carefully; Complete it appropriately. Note that the agreement should be signed by a full-time staff member (that is, the student is not acceptable). Please scan the signed agreement and send it to (ustc_vslrg At 126.com) and CC to Prof. Zhou (zhwg At ustc.edu.cn) and the full-time staff member who sign the agreement. Notation If you use this NMFs-CSL dataset in your research, please cite the following papers: Hezhen Hu, Wengang Zhou, Junfu Pu, and Houqiang Li, \u0026#34;Global-Local Enhancement Network for NMF-Aware Sign Language Recognition,\u0026#34; ACM Trans. Multimedia Comput. Commun. Appl. (TOMM), 2021. Hezhen Hu, Wengang Zhou, Junfu Pu, and Houqiang Li, \u0026#34;Hand-Model-Aware Sign Language Recognition,\u0026#34; AAAI Conference on Artificial Intelligence (AAAI), 2021. Hezhen Hu, Weichao Zhao, Wengang Zhou, and Houqiang Li, \u0026#34;SignBERT+: Hand-model-aware Self-supervised Pre-training for Sign Language Understanding,\u0026#34; IEEE Trans. Pattern Anal. Mach. Intell. (TPAMI), 2023. Hezhen Hu, Weichao Zhao, Wengang Zhou, Yuechen Wang, Houqiang Li, \u0026#34;SignBERT: Pre-Training of Hand-Model-Aware Representation for Sign Language Recognition,\u0026#34; International Conference on Computer Vision (ICCV), 2021. Contact If you have any questions about the dataset and our papers, please feel free to contact us: Houqiang Li, Professor, USTC, lihq AT ustc.edu.cn Wengang Zhou, Professor, USTC, zhwg AT ustc.edu.cn Weichao Zhao, Ph.D Student, USTC, saruka AT mail.ustc.edu.cn ","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"347f9fd18f9c7ed163570111e65fd4fc","permalink":"https://ustc-slr.github.io/datasets/2020_nmfs_csl/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/datasets/2020_nmfs_csl/","section":"datasets","summary":"","tags":null,"title":"NMFs-CSL Dataset","type":"datasets"},{"authors":["Zhihai Zhang","Junfu Pu","Liansheng Zhuang","Wengang Zhou","Houqiang Li"],"categories":null,"content":"","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572566400,"objectID":"3fce46833c4d435f1960afa2722ef2da","permalink":"https://ustc-slr.github.io/publication/zhang2019continuous/","publishdate":"2019-11-01T00:00:00Z","relpermalink":"/publication/zhang2019continuous/","section":"publication","summary":"In this paper, we propose an approach to apply the Transformer with reinforcement learning (RL) for continuous sign language recognition (CSLR) task. The Transformer has an encoder-decoder structure, where the encoder network encodes the sign video into the context vector representation, while the decoder network generates the target sentence word by word based on the context vector. To avoid the intrinsic defects of supervised learning (SL) in our task, e.g., the exposure bias and non-differentiable task metrics issues, we propose to train the Transformer directly on non-differentiable metrics, i.e., word error rate (WER), through RL. Moreover, a policy gradient algorithm with baseline, which we call Self-critic REINFORCE, is employed to reduce variance while training. Experimental results on RWTH-PHOENIX- Weather benchmark verify the effectiveness of our method and demonstrate that our method achieves the comparable performance.","tags":["CSLR"],"title":"Continuous sign language recognition via reinforcement learning","type":"publication"},{"authors":["Chengcheng Wei","Wengang Zhou","Junfu Pu","Houqiang Li"],"categories":null,"content":"","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572566400,"objectID":"7d7406bb956d34900f2b86ca43c8790a","permalink":"https://ustc-slr.github.io/publication/wei2019deep/","publishdate":"2019-11-01T00:00:00Z","relpermalink":"/publication/wei2019deep/","section":"publication","summary":"In this paper, we propose a novel deep architecture with multiple classifiers for continuous sign language recognition. Representing the sign video with a 3D convolutional residual network and a bidirectional LSTM, we formulate continuous sign language recognition as a grammatical-rule- based classification problem. We first split a text sentence of sign language into isolated words and n-grams, where an n- gram is a sequence of consecutive n words in a sentence. Then, we propose a word-independent classifiers (WIC) module and an n-gram classifier (NGC) module to identify the words and n-grams in a sentence, respectively. A greedy decoding algorithm is employed to integrate words and n-grams into the sentence based on the confidence scores provided by both modules. Our method is evaluated on a Chinese continuous sign language recognition benchmark, and the experimental results demonstrate its effectiveness and superiority.","tags":["CSLR"],"title":"Deep grammatical multi-classifier for continuous sign language recognition","type":"publication"},{"authors":["Hao Zhou","Wengang Zhou","Houqiang Li"],"categories":null,"content":"","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572566400,"objectID":"d27523f8a8c3f3b669ab5309a591aba6","permalink":"https://ustc-slr.github.io/publication/zhou2019dynamic/","publishdate":"2019-11-01T00:00:00Z","relpermalink":"/publication/zhou2019dynamic/","section":"publication","summary":"Continuous sign language recognition is a weakly supervised problem to translate video sequence to sign gloss sequence, where temporal boundary of each sign gloss is not annotated. The CNN-RNN-CTC framework shows effectiveness in this task by estimating pseudo label for each clip and retraining the feature extractor alternately. The quality of pseudo labels greatly impacts the final performance. In contrast of existing methods which select labels of maximum posterior probability, we propose a dynamic pseudo label decoding method to find a reasonable alignment path via dynamic-programming. Our approach filters out apparently wrong labels and generates pseudo labels which conform to natural word order of sign language. To further boost the performance after iterative optimization, we introduce a temporal ensemble module equipped with BGRU and 1D-CNN to integrate features from different time scales. Experiments on two continuous sign language benchmarks with large vocabulary show the effectiveness of our proposed method.","tags":["CSLR"],"title":"Dynamic pseudo label decoding for continuous sign language recognition","type":"publication"},{"authors":["Hao Zhou","Wengang Zhou","Yun Zhou","Houqiang Li"],"categories":null,"content":"","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572566400,"objectID":"e9c804422307d4a45492d4863296ca63","permalink":"https://ustc-slr.github.io/publication/zhou2020spatial/","publishdate":"2019-11-01T00:00:00Z","relpermalink":"/publication/zhou2020spatial/","section":"publication","summary":"Despite the recent success of deep learning in continuous sign language recognition (CSLR), deep models typically focus on the most discriminative features, ignoring other poten- tially non-trivial and informative contents. Such characteristic heavily constrains their capability to learn implicit visual grammars behind the collaboration of different visual cues (i,e., hand shape, facial expression and body posture). By injecting multi-cue learning into neural network design, we propose a spatial-temporal multi-cue (STMC) network to solve the vision-based sequence learning problem. Our STMC network consists of a spatial multi-cue (SMC) module and a temporal multi-cue (TMC) module. The SMC module is ded- icated to spatial representation and explicitly decomposes visual features of different cues with the aid of a self-contained pose estimation branch. The TMC module models temporal correlations along two parallel paths, i.e., intra-cue and inter- cue, which aims to preserve the uniqueness and explore the collaboration of multiple cues. Finally, we design a joint optimization strategy to achieve the end-to-end sequence learn- ing of the STMC network. To validate the effectiveness, we perform experiments on three large-scale CSLR benchmarks, i.e., PHOENIX-2014, CSL and PHOENIX-2014-T. Experimental results demonstrate that the proposed method achieves new state-of-the-art performance on all three benchmarks.","tags":["CSLR"],"title":"Spatial-temporal multi-cue network for continuous sign language recognition","type":"publication"},{"authors":["Junfu Pu","Wengang Zhou","Houqiang Li"],"categories":null,"content":"","date":1548979200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548979200,"objectID":"3f5ae501e886459c81ebcecb816ac417","permalink":"https://ustc-slr.github.io/publication/pu2019iterative/","publishdate":"2019-02-01T00:00:00Z","relpermalink":"/publication/pu2019iterative/","section":"publication","summary":"In this paper, we propose an alignment network with iterative optimization for weakly supervised continuous sign language recognition. Our framework consists of two modules a 3D convolutional residual network (3D-ResNet) for feature learning and an encoder-decoder network with connectionist temporal classification (CTC) for sequence modelling. The above two modules are optimized in an alternate way. In the encoder-decoder sequence learning network, two decoders are included, i.e., LSTM decoder and CTC decoder. Both decoders are jointly trained by maximum likelihood criterion with a soft Dynamic Time Warping (soft-DTW) alignment constraint. The warping path, which indicates the possible alignment between input video clips and sign words, is used to fine-tune the 3D-ResNet as training labels with classification loss. After fine-tuning, the improved features are extracted for optimization of encoder- decoder sequence learning network in next iteration. The proposed algorithm is evaluated on two large scale continuous sign language recognition benchmarks, i.e., RWTH- PHOENIX-Weather and CSL. Experimental results demonstrate the effectiveness of our proposed method.","tags":["CSLR"],"title":"Iterative alignment network for continuous sign language recognition","type":"publication"},{"authors":["Dan Guo","Wengang Zhou","Houqiang Li","Meng Wang"],"categories":null,"content":"","date":1541030400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541030400,"objectID":"5c16c88ebd61e45afc5ad07ce10be78e","permalink":"https://ustc-slr.github.io/publication/guo2018hierarchical/","publishdate":"2018-11-01T00:00:00Z","relpermalink":"/publication/guo2018hierarchical/","section":"publication","summary":"Continuous Sign Language Translation (SLT) is a challenging task due to its specific linguistics under sequential gesture variation without word alignment. Current hybrid HMM and CTC (Connectionist temporal classification) based mod- els are proposed to solve frame or word level alignment. They may fail to tackle the cases with messing word order corresponding to visual content in sentences. To solve the issue, this paper proposes a hierarchical-LSTM (HLSTM) encoder- decoder model with visual content and word embedding for SLT. It tackles different granularities by conveying spatio-temporal transitions among frames, clips and viseme units. It firstly explores spatio-temporal cues of video clips by 3D C- NN and packs appropriate visemes by online key clip mining with adaptive variable-length. After pooling on recurrent outputs of the top layer of HLSTM, a temporal attention-aware weighting mechanism is proposed to balance the intrinsic relationship among viseme source positions. At last, another two LSTM layers are used to separately recurse viseme vectors and translate semantic. After preserving original visual content by 3D CNN and the top layer of HLSTM, it shortens the encoding time step of the bottom two LSTM layers with less computational complexity while attaining more nonlinearity. Our proposed model exhibits promising performance on singer-independent test with seen sentences and also outperforms the comparison algorithms on unseen sentences.","tags":["SLT"],"title":"Hierarchical LSTM for sign language translation","type":"publication"},{"authors":["Jie Huang","Wengang Zhou","Qilin Zhang","Houqiang Li","Weiping Li"],"categories":null,"content":"","date":1541030400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541030400,"objectID":"cd6da5f8de0b75591185ed03f789682c","permalink":"https://ustc-slr.github.io/publication/huang2018video/","publishdate":"2018-11-01T00:00:00Z","relpermalink":"/publication/huang2018video/","section":"publication","summary":"Millions of hearing impaired people around the world routinely use some variants of sign languages to communicate, thus the automatic translation of a sign language is meaningful and important. Currently, there are two sub-problems in Sign Language Recognition (SLR), i.e., isolated SLR that recognizes word by word and continuous SLR that translates entire sentences. Existing continuous SLR methods typically utilize isolated SLRs as building blocks, with an extra layer of preprocessing (temporal segmentation) and another layer of post-processing (sentence synthesis). Unfortunately, temporal segmentation itself is non-trivial and inevitably propagates errors into subsequent steps. Worse still, isolated SLR methods typically require strenuous labeling of each word separately in a sentence, severely limiting the amount of attainable training data. To address these challenges, we propose a novel continuous sign recognition framework, the Hierarchical Attention Network with Latent Space (LS-HAN), which eliminates the preprocessing of temporal segmentation. The proposed LS-HAN consists of three components a two-stream Convolutional Neural Network (CNN) for video feature representation generation, a Latent Space (LS) for semantic gap bridging, and a Hierarchical Attention Network (HAN) for latent space based recognition. Experiments are carried out on two large scale datasets. Experimental results demonstrate the effectiveness of the proposed framework.","tags":["ISLR"],"title":"Video-based sign language recognition without temporal segmentation","type":"publication"},{"authors":["Shuo Wang","Dan Guo","Wengang Zhou","Zheng-Jun Zha","Meng Wang","Dan Guo"],"categories":null,"content":"","date":1539561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539561600,"objectID":"4cd575b301bec4eef022a84d0ce48853","permalink":"https://ustc-slr.github.io/publication/wang2018connectionist/","publishdate":"2018-10-15T00:00:00Z","relpermalink":"/publication/wang2018connectionist/","section":"publication","summary":"Continuous sign language translation (CSLT) is a weakly supervised problem aiming at translating vision-based videos into natural languages under complicated sign linguistics, where the ordered words in a sentence label have no exact boundary of each sign action in the video. This paper proposes a hybrid deep architecture which consists of a temporal convolution module (TCOV), a bidirectional gated recurrent unit module (BGRU), and a fusion layer module (FL) to address the CSLT problem. TCOV captures short-term temporal transition on adjacent clip features (local pattern), while BGRU keeps the long-term context transition across temporal dimension (global pattern). FL concatenates the feature embedding of TCOV and BGRU to learn their complementary relationship (mutual pattern). Thus we propose a joint connectionist temporal fusion (CTF) mechanism to utilize the merit of each module. The proposed joint CTC loss optimization and deep classification score-based decoding fusion strategy are designed to boost performance. With only once training, our model under the CTC constraints achieves comparable performance to other existing methods with multiple EM iterations. Experiments are tested and verified on a benchmark, i.e. the RWTH-PHOENIX-Weather dataset, which demonstrate the effectiveness of our proposed method.","tags":["SLT"],"title":"Connectionist Temporal Fusion for Sign Language Translation","type":"publication"},{"authors":["Jie Huang","Wengang Zhou","Houqiang Li","Weiping Li"],"categories":null,"content":"","date":1537056000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1537056000,"objectID":"3d7e08de1ff7d00aed373f7a40bc2b06","permalink":"https://ustc-slr.github.io/publication/huang2018attention/","publishdate":"2018-09-16T00:00:00Z","relpermalink":"/publication/huang2018attention/","section":"publication","summary":"Sign language recognition (SLR) is an important and challenging research topic in the multimedia field. Conventional techniques for SLR rely on hand-crafted features, which achieve limited success. In this paper, we present attention-based 3D-convolutional neural networks (3D-CNNs) for SLR. The framework has two advantages 3D-CNNs learn spatio-temporal features from raw video without prior knowledge and the attention mechanism helps to select the clue. When training 3D-CNN for capturing spatio-temporal features, spatial attention is incorporated into the network to focus on the areas of interest. After feature extraction, temporal attention is utilized to select the significant motions for classification. The proposed method is evaluated on two large scale sign language data sets. The first one, collected by ourselves, is a Chinese sign language data set that consists of 500 categories. The other is the ChaLearn14 benchmark. The experiment results demonstrate the effectiveness of our approach compared with state-of-the-art algorithms.","tags":["ISLR"],"title":"Attention-Based 3D-CNNs for Large-Vocabulary Sign Language Recognition","type":"publication"},{"authors":["Junfu Pu","Wengang Zhou","Houqiang Li"],"categories":null,"content":"","date":1517443200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1517443200,"objectID":"9c01a5e8d6fe510605006ca68e57a8cf","permalink":"https://ustc-slr.github.io/publication/pu2018dilated/","publishdate":"2018-02-01T00:00:00Z","relpermalink":"/publication/pu2018dilated/","section":"publication","summary":"This paper presents a novel deep neural architecture with iterative optimization strategy for realworld continuous sign language recognition. Generally, a continuous sign language recognition system consists of visual input encoder for feature extraction and a sequence learning model to learn the correspondence between the input sequence and the output sentence-level labels. We use a 3D residual convolutional network (3D-ResNet) to extract visual features. After that, a stacked dilated convolutional network with Connectionist Temporal Classification (CTC) is applied for learning the mapping between the sequential features and the text sentence. The deep network is hard to train since the CTC loss has limited contribution to early CNN parameters. To alleviate this problem, we design an iterative optimization strategy to train our architecture. We generate pseudo-labels for video clips from sequence learning model with CTC, and finetune the 3D-ResNet with the supervision of pseudo-labels for a better feature representation. We alternately optimize feature extractor and sequence learning model with iterative steps. Experimental results on RWTH-PHOENIX-Weather, a large real-world continuous sign language recognition benchmark, demonstrate the advantages and effectiveness of our proposed method.","tags":["CSLR"],"title":"Dilated convolutional network with iterative optimization for continuous sign language recognition","type":"publication"},{"authors":["Junfu Pu","Wengang Zhou","Houqiang Li"],"categories":null,"content":"","date":1480204800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1480204800,"objectID":"bd199b1a55ed5db786c389956570439e","permalink":"https://ustc-slr.github.io/publication/pu2016sign2/","publishdate":"2016-11-27T00:00:00Z","relpermalink":"/publication/pu2016sign2/","section":"publication","summary":"We study the problem of recognizing sign language automatically using the RGB videos and skeleton coordinates captured by Kinect, which is of great significance in communication between the deaf and the hearing societies. In this paper, we propose a sign language recognition (SLR) system with data of two channels, including the gesture videos of the sign words and joint trajectories. In our framework, we extract two modals of features to represent the hand shape videos and hand trajectories for recognition. The variation of gesture is obtained by 3D CNN and the activations of fully connected layers are used as the representations of these sign videos. For trajectories, we use the shape context to describe each joint, and combine them all within a feature matrix. After that, a convolutional neural network is applied to generate a robust representation of these trajectories. Furthermore, we fuse these features and train a SVM classifier for recognition. We conduct some experiments on large vocabulary sign language dataset with up to 500 words and the results demonstrate the effectiveness of our proposed method.","tags":["ISLR"],"title":"Sign Language Recognition with Multi-modal Features","type":"publication"},{"authors":["Dan Guo","Wengang Zhou","Meng Wang","Houqiang Li"],"categories":null,"content":"","date":1475020800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1475020800,"objectID":"95bbcb34186e3c6d8d905e4f7e3efa81","permalink":"https://ustc-slr.github.io/publication/guo2016sign/","publishdate":"2016-09-28T00:00:00Z","relpermalink":"/publication/guo2016sign/","section":"publication","summary":"Vision based sign language recognition (SLR) is a challenging task due to the complexity of signs and limited data collection. To improve the recognition precision, this paper proposes an adaptive GMM-based (Gaussian mixture model) HMMs (Hidden Markov Models) framework. We discover that inherent latent states in HMMs are not only related to the number of key gestures and body poses, but also related to the kinds of their translation relationships. We propose adaptive HMMs and obtain the hidden state number for each sign with affinity propagation clustering. Furthermore, to enrich the training dataset, we propose a data augmentation strategy by adding Gaussian random disturbances. Experiments on a vocabulary of 370 signs demonstrate the effectiveness of our proposed method over the comparison algorithms.","tags":["ISLR"],"title":"Sign language recognition based on adaptive HMMs with data augmentation","type":"publication"},{"authors":["Tao Liu","Wengang Zhou","Houqiang Li"],"categories":null,"content":"","date":1475020800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1475020800,"objectID":"de11583eaf01b6c5a370bbf0e0c01bf0","permalink":"https://ustc-slr.github.io/publication/liu2016sign/","publishdate":"2016-09-28T00:00:00Z","relpermalink":"/publication/liu2016sign/","section":"publication","summary":"Sign Language Recognition (SLR) aims at translating the Sign Language (SL) into speech or text, so as to facilitate the communication between hearing-impaired people and the normal people. This problem has broad social impact, however it is challenging due to the variation for different people and the complexity in sign words. Traditional methods for SLR generally use handcrafted feature and Hidden Markov Models (HMMs) modeling temporal information. But reliable handcrafted features are difficult to design and not able to adapt to the large variations of sign words. To approach this problem, considering that Long Short-Term memory (LSTM) can model the contextual information of temporal sequence well, we propose an end-to-end method for SLR based on LSTM. Our system takes the moving trajectories of 4 skeleton joints as inputs without any prior knowledge and is free of explicit feature design. To evaluate our proposed model, we built a large isolated Chinese sign language vocabulary with Kinect 2.0. Experimental results demonstrate the effectiveness of our approach compared with traditional HMM based methods.","tags":["ISLR"],"title":"Sign language recognition with long short-term memory","type":"publication"},{"authors":["Jihai Zhang","Wengang Zhou","Chao Xie","Junfu Pu","Houqiang Li"],"categories":null,"content":"","date":1468195200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1468195200,"objectID":"6e823b660c0c61e47e86678cc62f399b","permalink":"https://ustc-slr.github.io/publication/zhang2016chinese/","publishdate":"2016-07-11T00:00:00Z","relpermalink":"/publication/zhang2016chinese/","section":"publication","summary":"Sign Language Recognition (SLR) aims at translating the sign language into text or speech, so as to realize the communication between deaf-mute people and ordinary people. This paper proposes a framework based on the Hidden Markov Models (HMMs) benefited from the utilization of the trajectories and hand-shape features of the original sign videos, respectively. First, we propose a new trajectory feature (enhanced shape context), which can capture the spatio-temporal information well. Second, we fetch the hand regions by Kinect mapping functions and describe each frame by HOG (pre-processed by PCA). Moreover, in order to optimize predictions, rather than fixing the number of hidden states for each sign model, we independently determine it through the variation of the hand shapes. As for recognition, we propose a combination method to fuse the probabilities of trajectory and hand shape. At last, we evaluate our approach with our self-building Kinect-based dataset and the experiments demonstrate the effectiveness of our approach.","tags":["ISLR"],"title":"Chinese sign language recognition with adaptive HMM","type":"publication"},{"authors":["Junfu Pu","Wengang Zhou","Jihai Zhang","Houqiang Li"],"categories":null,"content":"","date":1451779200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451779200,"objectID":"05d238340d1af366e06895b421589af6","permalink":"https://ustc-slr.github.io/publication/pu2016sign/","publishdate":"2016-01-03T00:00:00Z","relpermalink":"/publication/pu2016sign/","section":"publication","summary":"Sign language recognition targets on interpreting and understanding the sign language for convenience of communication between the deaf and the normal people, which has broad social impact. The problem is challenging due to the large variations for different signers and the subtle difference between sign words. In this paper, we propose a new method for isolated sign language recognition based on trajectory modeling with hidden Markov models (HMMs). In our approach, we first normalize and re-sample the raw trajectory data and partition the trajectory into multiple segments. To represent each trajectory segment, we proposed a new curve feature descriptor based on shape context. After that, hidden Markov model is used to model each isolated sign word for recognition. To evaluate the performance of our proposed algorithm, we have built a large isolated Chinese sign language vocabulary with Kinect 2.0. The dataset contains 100 unique isolated sign words, each of which is performed by 50 signers for 5 times. Experimental results demonstrate that the proposed method achieves a better performance compared with normal coordinate feature with HMM.","tags":["ISLR"],"title":"Sign Language Recognition Based on Trajectory Modeling with HMMs","type":"publication"},{"authors":["Jihai Zhang","Wengang Zhou","Houqiang Li"],"categories":null,"content":"","date":1436659200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1436659200,"objectID":"d7e93b779f12710336cdc98900ee4e13","permalink":"https://ustc-slr.github.io/publication/zhang2015new/","publishdate":"2015-07-12T00:00:00Z","relpermalink":"/publication/zhang2015new/","section":"publication","summary":"In this paper, we propose a new system for isolated sign language recognition (SLR) and continuous SLR. In isolated SLR, Histogram of Oriented Displacement is used to describe the trajectories, and multi-SVM is adopted for classification. In continuous SLR, we propose a Dynamic Programming method with warping templates obtained by Dynamic Time Warping (DTW) algorithm. We evaluate our approach with 450 phrases and 180 sentences recorded by Kinect and compare with classical methods, including Hidden Markov Models and state-of-the-art Conditional Random Fields (CRF), Hidden CRF and Latent Dynamic CRF. The experiments demonstrate the effectiveness of our method.","tags":["ISLR"],"title":"A new system for Chinese sign language recognition","type":"publication"},{"authors":["Jie Huang","Wengang Zhou","Houqiang Li","Weiping Li"],"categories":null,"content":"","date":1436659200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1436659200,"objectID":"252eb1bef7847410aa4c4f8922e8f802","permalink":"https://ustc-slr.github.io/publication/huang2015sign2/","publishdate":"2015-07-12T00:00:00Z","relpermalink":"/publication/huang2015sign2/","section":"publication","summary":"Sign Language Recognition (SLR) targets on facilitating the communication between deaf-mute people and ordinary people. This task is very challenging due to the complexity and large variations in hand postures. Some methods require user wear sensor gloves which can detect the position and angle of finger articulations. Others use RGB-D camera like Kinect to track hands and rely on complex algorithms to segment hands from background. However, all these methods have its own disadvantages. Sensor-based methods are not natural as the user must wear cumbersome instruments while camera-based methods have to design extra algorithms to track and segment hands from complex background. To address these problems, we propose a novel method for SLR which involves the use of the Real-Sense. It is a camera device which can detect and track the location of hands in a natural way. More powerful, it provides the 3D coordinates of finger joints in real time. We build a deep neural network (DNN) based on Real-Sense to recognize different signs. The DNN takes the 3D coordinates of finger joints as input directly without using any handcrafted features. The reason is that DNN, as a deep model, is capable of learning suitable features for recognition from raw data. In experiment, to demonstrate the effectiveness of Real-Sense, we collect two datasets by Real-Sense and Kinect respectively, then build DNNs based on each dataset for recognition. To validate the powerfulness of DNN, we compare the performance of DNN and support vector machine (SVM) on the same dataset.","tags":["ISLR"],"title":"Sign Language Recognition using Real-Sense","type":"publication"},{"authors":["Jie Huang","Wengang Zhou","Houqiang Li","Weiping Li"],"categories":null,"content":"","date":1435536000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1435536000,"objectID":"6c2d9a4ad14bcd8887f9ef5466b5ddee","permalink":"https://ustc-slr.github.io/publication/huang2015sign/","publishdate":"2015-06-29T00:00:00Z","relpermalink":"/publication/huang2015sign/","section":"publication","summary":"Sign Language Recognition (SLR) targets on interpreting the sign language into text or speech, so as to facilitate the communication between deaf-mute people and ordinary people. This task has broad social impact, but is still very challenging due to the complexity and large variations in hand actions. Existing methods for SLR use hand-crafted features to describe sign language motion and build classification models based on those features. However, it is difficult to design reliable features to adapt to the large variations of hand gestures. To approach this problem, we propose a novel 3D convolutional neural network (CNN) which extracts discriminative spatial-temporal features from raw video stream automatically without any prior knowledge, avoiding designing features. To boost the performance, multi-channels of video streams, including color information, depth clue, and body joint positions, are used as input to the 3D CNN in order to integrate color, depth and trajectory information. We validate the proposed model on a real dataset collected with Microsoft Kinect and demonstrate its effectiveness over the traditional approaches based on hand-crafted features.","tags":["ISLR"],"title":"Sign Language Recognition using 3D convolutional neural networks","type":"publication"},{"authors":null,"categories":null,"content":"Download (*Important ) The CSL databases are released to universities and research institutes for research purpose only. To request the access right to the data resources, please follow the instructions below: Download the CSL Dataset Release Agreement; Read it carefully; Complete it appropriately. Note that the agreement should be signed by a full-time staff member (that is, the student is not acceptable). Please scan the signed agreement and send it to (ustc_vslrg At 126.com) and CC to Prof. Zhou (zhwg At ustc.edu.cn) and the full-time staff member who sign the agreement. Introduction We have two Chinese sign language datasets for isolated Sign Language Recognition and continuous Sign Language Recognition, respectively. Both datasets are collected with Kinect 2.0 by 50 signers. Each signer perfors 5 times for every word (sentence). The sign videos are recorded with 30fps. The distance between the signers and Kinect is about 1.5 meters. Each instance in both datasets contains RGB videos, depth videos, and 3D joints information of the signer. Since the dataset is recorded with Microsoft Kinect, there are three data modalities available: RGB videos with resolution of 1280 x 720 pixels and frame rate of 30 fps. Depth videos with resolution of 512x 424 pixels and frame rate of 30 fps. Twenty-five skeleton joints locations of each frame. Isolated SLR The isolated SLR dataset contains 500 Chinese sign words. Each sign video is performed by 50 signers with 5 times. Hence, there are 250 instances for each sign word. If you use this Chinese isolated SLR dataset in your research, please consider citing the following papers: Hezhen Hu, Weichao Zhao, Wengang Zhou, and Houqiang Li, \u0026#34;SignBERT+: Hand-model-aware Self-supervised Pre-training for Sign Language Understanding,\u0026#34; IEEE Trans. Pattern Anal. Mach. Intell. (TPAMI), 2023.\tWeichao Zhao, Hezhen Hu, Wengang Zhou, Jiaxin Shi, and Houqiang Li, \u0026#34;BEST: BERT Pre-Training for Sign Language Recognition with Coupling Tokenization,\u0026#34; Conference on Artificial Intelligence (AAAI), 2023. Jie Huang, Wengang Zhou, Houqiang Li, and Weiping Li, \u0026#34;Attention based 3D-CNNs for Large-Vocabulary Sign Language Recognition, IEEE Trans. Circuits Syst. Video Technol. (TCSVT), 2018. Continuous SLR The corpus of continuous SLR dataset contains 100 Chinese sentence. There are 250 instances (50signers x 5times) for each sentence. If you use this Chinese continuous SLR dataset in your research, please consider citing the following papers: Junfu Pu, Wengang Zhou, and Houqiang Li, \u0026#34;Iterative Alignment Network for Continuous Sign Language Recognition,\u0026#34; Conference on Computer Vision and Pattern Recognition (CVPR), 2019. Hao Zhou, Wengang Zhou, and Houqiang Li, \u0026#34;Dynamic Pseudo Label Decoding for Continuous Sign Language Recognition,\u0026#34; International Conference on Multimedia and Expo (ICME), 2019. Jie Huang, Wengang Zhou, Qilin Zhang, Houqiang Li and Weiping Li, \u0026#34;Video-based Sign Language Recognition without Temporal Segmentation,\u0026#34; AAAI Conference on Artificial Intelligence (AAAI), 2018. Besides, you can refer to the following papers for continuous SLR published by our group: Hezhen Hu, Weichao Zhao, Wengang Zhou, and Houqiang Li, \u0026#34;SignBERT+: Hand-model-aware Self-supervised Pre-training for Sign Language Understanding,\u0026#34; IEEE Trans. Pattern Anal. Mach. Intell. (TPAMI), 2023.\tHezhen Hu, Junfu Pu, Wengang Zhou, and Houqiang Li, \u0026#34;Collaborative Multilingual Continuous Sign Language Recognition: A Unified Framework,\u0026#34; IEEE Trans. Multimedia (TMM), 2022. Junfu Pu, Wengang Zhou, Hezhen Hu, and Houqiang Li, \u0026#34;Boosting Continuous Sign Language Recognition via Cross Modality Augmentation,\u0026#34; ACM International Conference on Multimedia (ACM MM), 2020. Contact If you have any questions about the dataset and our papers, please feel free to contact us: Houqiang Li, Professor, USTC, lihq AT ustc.edu.cn Wengang Zhou, Professor, USTC, zhwg AT ustc.edu.cn Weichao Zhao, Ph.D Student, USTC, saruka AT mail.ustc.edu.cn ","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"ff508db497487265c5a1ba9baaea652b","permalink":"https://ustc-slr.github.io/datasets/2015_csl/","publishdate":"2015-01-01T00:00:00Z","relpermalink":"/datasets/2015_csl/","section":"datasets","summary":"","tags":null,"title":"CSL \u0026 SLR500 Dataset","type":"datasets"},{"authors":["Jihai Zhang","Wengang Zhou","Houqiang Li"],"categories":null,"content":"","date":1404950400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1404950400,"objectID":"b26e333a53d808ec266411d4db44a78f","permalink":"https://ustc-slr.github.io/publication/zhang2014threshold/","publishdate":"2014-07-10T00:00:00Z","relpermalink":"/publication/zhang2014threshold/","section":"publication","summary":"Recently, great progress has been made in sign language recognition. Most approaches are based on the Hidden Markov Model (HMM) with various features, such as motion trajectory. Recognition for sign sentences is obtained from optimal path by Viterbi algorithm, however, some wrong jumps are usually caused by transitional movements between signs. To address the problem, in this paper, we propose an approach consisting of two stages offline training and online recognition. In the offline training stage, we propose a threshold matrix and rate thresholds. Each element of the threshold matrix describes the minimal probability when a segment belongs to a sign, and rate thresholds are defined as the average probability for signs. So, if certain segment's evaluation is smaller than all the thresholds, it is regarded as a transitional movement and then it should be removed. In the online recognition stage, coarse segmentation, based on the threshold matrix, records the time interval for fine segmentation, and fine segmentation, based on Dynamic Time Warping(DTW) and Length-Root method, determines the endpoint for each candidate sign and selects the most possible one. The final recognition is obtained by concatenating the most possible signs. We evaluate our approach with Kinect-based dataset and the experiments demonstrate the effectiveness of our approach.","tags":["CSLR"],"title":"A Threshold-based HMM-DTW Approach for Continuous Sign Language Recognition","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c1d17ff2b20dca0ad6653a3161942b64","permalink":"https://ustc-slr.github.io/people/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/people/","section":"","summary":"","tags":null,"title":"","type":"widget_page"}]